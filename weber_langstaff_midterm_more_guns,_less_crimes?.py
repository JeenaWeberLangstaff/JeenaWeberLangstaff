# -*- coding: utf-8 -*-
"""Weber Langstaff Midterm: More Guns, Less Crimes?

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pnoAjXUmdc5Bw78Hh1erLA4j-FgF9WQm

#More Guns, Less Crime?

### More Guns, Less Crimes Dataset:

### This dataset contains state-level panel data from 1977 to 1999, used to study the relationship between gun laws and crime rates in the United States.It's a data frame containing 1,173 observations on 13 variables. The data source is AER package- Rdatasets. https://vincentarelbundock.github.io/Rdatasets/doc/AER/Guns.html. The target variable is the law, whether a right-to-carry gun law was in effect. All independent variables are observed per 100,000 population. The independent variables are  year (observation year), violent (crime rate), murder (murder rate), robbery (robbery rate), prisoners (number of prisoners), afam (percentage of African Americans), cauc (percentage of Caucasian population), male (population), populations (in the millions), income (real per capita income), density (population density per square mile), and state (U.S state- used per grouping not prediction). There are a total of 51 states (including Washington D.C) times 23 years = 1,173 observations. The task type is classification, to predict whether a right-to-carry law (law) was in effect based on the numerical features as a "yes" or "no."

### Data Cleaning and Preprocessing:
"""

from google.colab import drive
import os

# 1. Mount Google Drive
drive.mount('/content/drive')

# Define the source path in your Google Drive
file_path_in_drive = '/content/drive/My Drive/Guns.csv'

# Define the destination path in your Colab environment with the new filename
destination_path_in_colab = '/content/Guns.csv'

# 2. Check if the file exists in Google Drive (optional but recommended)
if os.path.exists(file_path_in_drive):
    # Copy the file and rename it
    !cp "{file_path_in_drive}" "{destination_path_in_colab}"
    print(f"File copied and renamed to {destination_path_in_colab}")
else:
    print(f"File not found at {file_path_in_drive}")

# Verify the file exists in the Colab environment
!ls -l /content/Guns.csv

# Define the path to the file in your Colab environment
file_path_in_colab = '/content/Guns.csv'

# Open and read the file
try:
    with open(file_path_in_colab, 'r') as f:
        file_content = f.read()
        print("File content:")
        print(file_content)
except FileNotFoundError:
    print(f"Error: File not found at {file_path_in_colab}")
except Exception as e:
    print(f"An error occurred: {e}")

import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder

# Load the dataset into a DataFrame
df = pd.read_csv('/content/Guns.csv')

# Display the first few rows
print(df.head())

# Check for missing values
print("\nMissing values:\n", df.isnull().sum())

# Ensure numeric columns are properly typed
numeric_columns = ['year', 'violent', 'murder', 'robbery', 'prisoners',
                   'afam', 'cauc', 'male', 'population', 'income', 'density']

df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')

# Impute missing values using mean strategy
imputer = SimpleImputer(strategy='mean')
df[numeric_columns] = imputer.fit_transform(df[numeric_columns])

# Encode the target variable 'law'
label_encoder = LabelEncoder()
df['law_encoded'] = label_encoder.fit_transform(df['law'])

# Preview the cleaned dataset
print("\nCleaned and encoded dataset:\n", df[['law', 'law_encoded']].drop_duplicates())

"""###Preliminary Data Analysis"""

import pandas as pd

# Load the dataset
df = pd.read_csv('/content/Guns.csv')

# Print the number of rows and columns
print(f"Number of rows: {df.shape[0]}")
print(f"Number of columns: {df.shape[1]}")

# Display basic statistics
print("\nBasic Statistical Summary:")
print(df.describe())

# Display non-null value counts
print("\nNon-null Value Counts:")
print(df.count())

# Summary of findings
print("\nSummary of Findings:")
print("- The dataset contains data from 1,173 observations and 14 columns.")
print("- The dataset spans from 1977 to 1999, providing over two decades of data.")
print("- All columns have 1,173 non-null values, indicating no missing data.")
print("- Variables like 'violent', 'murder', and 'robbery' show wide ranges, suggesting variability in crime rates across states and years.")
print("- 'afam' and 'cauc' represent demographic percentages, while 'income' and 'population' vary significantly across states.")
print("- The 'prisoners' column shows a general increase over time, which may reflect changes in incarceration policies or crime rates.")

"""###Exploratory Analysis

####Scatter Plots
"""

# Import necessary libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('Guns.csv')

# Set the visual style
sns.set(style="whitegrid")

# Scatter Plot: Income vs Violent Crime
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='income', y='violent', hue='law')
plt.title('Income vs Violent Crime')
plt.xlabel('Income')
plt.ylabel('Violent Crime Rate')
plt.legend(title='Gun Law')
plt.grid(True)
plt.tight_layout()
plt.show()

# Scatter Plot: Population vs Murder Rate
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='population', y='murder', hue='law')
plt.title('Population vs Murder Rate')
plt.xlabel('Population')
plt.ylabel('Murder Rate')
plt.legend(title='Gun Law')
plt.grid(True)
plt.tight_layout()
plt.show()

"""####Bar Graphs"""

# Import necessary libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('Guns.csv')

# Set the visual style and color palette
sns.set(style="whitegrid")
palette = {"yes": "#2a9d8f", "no": "#e76f51"}

# Function to annotate bars
def annotate_bars(ax):
    for p in ax.patches:
        height = p.get_height()
        ax.annotate(f'{height:.1f}',
                    (p.get_x() + p.get_width() / 2., height),
                    ha='center', va='bottom', fontsize=10, color='black')

# Bar Plot: Average Violent Crime by Gun Law
plt.figure(figsize=(8, 6))
ax1 = sns.barplot(data=df, x='law', y='violent', estimator='mean', palette=palette)
annotate_bars(ax1)
plt.title('Average Violent Crime by Gun Law', fontsize=14, weight='bold')
plt.xlabel('Gun Law', fontsize=12)
plt.ylabel('Average Violent Crime Rate', fontsize=12)
plt.tight_layout()
plt.show()

# Bar Plot: Average Murder Rate by Gun Law
plt.figure(figsize=(8, 6))
ax2 = sns.barplot(data=df, x='law', y='murder', estimator='mean', palette=palette)
annotate_bars(ax2)
plt.title('Average Murder Rate by Gun Law', fontsize=14, weight='bold')
plt.xlabel('Gun Law', fontsize=12)
plt.ylabel('Average Murder Rate', fontsize=12)
plt.tight_layout()
plt.show()

# Bar Plot: Average Robbery Rate by Gun Law
plt.figure(figsize=(8, 6))
ax3 = sns.barplot(data=df, x='law', y='robbery', estimator='mean', palette=palette)
annotate_bars(ax3)
plt.title('Average Robbery Rate by Gun Law', fontsize=14, weight='bold')
plt.xlabel('Gun Law', fontsize=12)
plt.ylabel('Average Robbery Rate', fontsize=12)
plt.tight_layout()
plt.show()

"""####Pie Chart"""

import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('Guns.csv')

# Count the values of the 'law' column
law_counts = df['law'].value_counts()

# Define custom colors and explode settings
colors = ['#264653', '#e76f51']  # Dark teal for 'yes', red-orange for 'no'
explode = (0.15, 0)  # Explode the 'yes' slice more

# Create the pie chart
plt.figure(figsize=(6, 6))
wedges, texts, autotexts = plt.pie(
    law_counts,
    labels=law_counts.index,
    autopct='%1.1f%%',
    startangle=140,
    colors=colors,
    explode=explode,
    shadow=True,
    textprops={'color': 'black'}
)

# Set white text for the larger slice (assumed to be 'no')
if law_counts[0] > law_counts[1]:
    autotexts[0].set_color('white')
else:
    autotexts[1].set_color('white')

# Equal aspect ratio ensures that pie is drawn as a circle
plt.axis('equal')

# Move the title upward to avoid overlap
plt.title('Distribution of Gun Law Status', y=1.08)

plt.tight_layout()
plt.show()

"""Density Plots"""

# Import necessary libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('Guns.csv')

# Set the visual style
sns.set(style="whitegrid")

# Define key variables and colors
key_vars = ['violent', 'murder', 'robbery', 'income', 'population']
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']

# Create density plots
plt.figure(figsize=(10, 6))
for var, color in zip(key_vars, colors):
    sns.kdeplot(df[var], label=var.capitalize(), fill=True, color=color, alpha=0.5)

# Customize the plot
plt.title('Density Plots of Key Variables', fontsize=14, weight='bold')
plt.xlabel('Value')
plt.ylabel('Density')
plt.legend(title='Variable')
plt.tight_layout()
plt.show()

# Import necessary libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('Guns.csv')

# Set the visual style
sns.set(style="whitegrid")

# Define key variables for density plots
key_vars = ['violent', 'murder', 'robbery', 'income', 'population']

# Create density plots grouped by 'law'
for var in key_vars:
    plt.figure(figsize=(8, 5))
    sns.kdeplot(data=df, x=var, hue='law', fill=True, common_norm=False, alpha=0.5)
    plt.title(f'Density Plot of {var.capitalize()} by Gun Law Status', fontsize=14, weight='bold')
    plt.xlabel(var.capitalize())
    plt.ylabel('Density')
    plt.legend(title='Gun Law')
    plt.tight_layout()
    plt.show()

"""####Scatter Matrix"""

# Import necessary libraries
import pandas as pd
from pandas.plotting import scatter_matrix
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('Guns.csv')

# Select key variables for the scatter matrix
scatter_vars = ['violent', 'murder', 'robbery', 'income', 'population']

# Create the scatter matrix
scatter_matrix(df[scatter_vars], figsize=(12, 10), diagonal='kde')

# Add a title
plt.suptitle('Scatter Matrix of Key Variables', y=1.02)
plt.show()

"""####Boxplot Matrix"""

# Import necessary libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('Guns.csv')

# Set the visual style
sns.set(style="whitegrid")

# Define key variables for the boxplot matrix
key_vars = ['violent', 'murder', 'robbery', 'income', 'population']

# Create a grid of boxplots
fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 10))
axes = axes.flatten()

# Generate boxplots for each variable grouped by 'law'
for i, var in enumerate(key_vars):
    sns.boxplot(data=df, x='law', y=var, ax=axes[i], palette='viridis') # Changed palette here
    axes[i].set_title(f'{var.capitalize()} by Gun Law Status', fontsize=12)
    axes[i].set_xlabel('Gun Law')
    axes[i].set_ylabel(var.capitalize())

# Hide any unused subplot axes
for j in range(len(key_vars), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

data_findings_discussion = """
### Diving into the Data: What's the Story with Guns and Crime?

#### 1. First Off: The Gun Law Divide

* The 'law' variable (whether a state has gun laws) is imbalanced:
    * 75.7% (888 observations) are from states without gun laws.
    * 24.3% (285 observations) are from states with gun laws.
* This imbalance, visible in the bar graph and pie chart, means conclusions are heavily influenced by data from states without gun laws.

#### 2. How Our Key Variables Are Spread Out

* Crime Rates (Violent, Murder, Robbery):
    * Density plots show these are "right-skewed."
    * This means most observations have lower crime rates, with fewer instances of very high rates.
* Income:
    * Distribution appears fairly centered, a bit like a bell curve, with a slight tilt towards higher incomes.
* Population:
    * Highly "right-skewed," indicating many observations from smaller states and fewer from very large ones.

#### 3. What's Connected to What? (Looking at the Relationships)

* Pairwise Scatter Plots:**
    * Strong positive correlations exist among violent, murder, and robbery crime rates (if one's high, others tend to be too).
    * No clear, straightforward linear links were found between crime rates and income or population; more complex interactions are likely at play.

* Violent Crime and Everything Else (with Gun Laws in Mind):**
    * Violent Crime vs. Murder/Robbery:Confirmed strong connections; gun law status didn't fundamentally alter these relationships.
    * Violent Crime vs. Income/Population: No obvious linear trends. States with and without gun laws were mixed across all income and population levels.

#### 4. The Big Question: Gun Laws and Crime (The Boxplots)

* Crime Rates (Violent, Murder, Robbery):
    * States with gun laws ('yes') generally showed **higher median crime rates** for violent crime, murder, and robbery compared to states *without* gun laws ('no').
    * The spread (variability) of crime rates also seemed larger in states with gun laws.
    * Crucial Point:This is an *observation*, not causation. It's likely states implement stricter gun laws *in response* to higher existing crime rates, rather than the laws causing more crime. Deeper analysis controlling for other factors is needed.

* **Income and Population:**
    * States with gun laws ('yes') tended to have a **slightly higher median income and a broader range of incomes.
    * Similarly, these states ('yes') often had higher median populations and a wider variety of population sizes, suggesting potential confounding.

#### My Takeaway

* While individual variables have their own stories, the direct link between gun laws and crime rates isn't simple.
* The data imbalance and the initial observation of higher crime rates in 'gun law' states are potentially misleading.
* This strongly hints at other factors (confounding variables) and reverse causality (crime leading to laws, not laws leading to crime).
* To truly understand, more advanced statistical models (e.g., regression analysis) are needed to untangle these complex relationships, accounting for socio-economic conditions, population, and temporal changes.
"""


print(data_findings_discussion)

"""###Decision Tree"""

features = ['year', 'violent', 'murder', 'robbery', 'prisoners', 'afam',
            'cauc', 'male', 'population', 'income', 'density']
X = df[features]
y = df['law'] # Target variable

"""####Unrestricted Tree"""

import matplotlib.pyplot as plt
from sklearn.tree import export_text, plot_tree

# IMPORTANT: This code block assumes 'model_unrestricted' and 'features'
# have been defined from running the main analysis script.

print("\n--- Visualizing Unrestricted High Complexity Decision Tree ---")
# Due to the high complexity, this plot might be very large and hard to read.
# We'll limit its depth for visualization purposes to make it somewhat viewable.
plt.figure(figsize=(20, 15))
plot_tree(model_unrestricted,
          feature_names=features,
          class_names=['no_law', 'yes_law'], # Map back to original labels
          filled=True,
          rounded=True,
          fontsize=8,
          max_depth=5) # Limiting depth for *visualization* only for readability
plt.title('Unrestricted High Complexity Decision Tree (Visualized with max_depth=5 for readability)')
plt.show()

# Print text representation of the unrestricted model's rules (first few levels for brevity)
print("\n--- Text Representation of Unrestricted High Complexity Model Rules (Top 5 Levels) ---")
# Exporting the full text can be extremely long, so we'll just show the top few levels
tree_rules_unrestricted = export_text(model_unrestricted, feature_names=features, class_names=['no_law', 'yes_law'], max_depth=5)
print(tree_rules_unrestricted)

"""####Low Complexity Tree"""

import matplotlib.pyplot as plt
from sklearn.tree import export_text, plot_tree

# IMPORTANT: This code block assumes 'model_low_complexity' and 'features'
# have been defined from running the main analysis script.

print("\n--- Visualizing Low Complexity Decision Tree (max_depth=3) ---")
plt.figure(figsize=(15, 10))
plot_tree(model_low_complexity,
          feature_names=features,
          class_names=['no_law', 'yes_law'], # Map back to original labels
          filled=True,
          rounded=True,
          fontsize=10)
plt.title('Low Complexity Decision Tree (max_depth=3)')
plt.show()

# Print text representation of the low complexity model's rules
print("\n--- Text Representation of Low Complexity Model Rules ---")
tree_rules = export_text(model_low_complexity, feature_names=features, class_names=['no_law', 'yes_law'])
print(tree_rules)

"""####High Complexity Tree"""

import matplotlib.pyplot as plt
from sklearn.tree import export_text, plot_tree

# IMPORTANT: This code block assumes 'model_optimized_complexity' and 'features'
# have been defined from running the main analysis script.

print("\n--- Visualizing Optimized Complexity Decision Tree (max_depth=7) ---")
plt.figure(figsize=(18, 12)) # Slightly larger figure for potentially deeper tree
plot_tree(model_optimized_complexity,
          feature_names=features,
          class_names=['no_law', 'yes_law'], # Map back to original labels
          filled=True,
          rounded=True,
          fontsize=9)
plt.title('Optimized Complexity Decision Tree (max_depth=7)')
plt.show()

# Print text representation of the optimized complexity model's rules (top levels)
print("\n--- Text Representation of Optimized Complexity Model Rules (Top Levels) ---")
# Limiting depth for text export for readability, adjust as needed
tree_rules_optimized = export_text(model_optimized_complexity, feature_names=features, class_names=['no_law', 'yes_law'], max_depth=7)
print(tree_rules_optimized)

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

train_scores = {
    "Unrestricted": 0.999,
    "Low Complexity (max_depth=3)": 0.760,
    "Optimized Complexity (max_depth=7)": 0.850
}
test_scores = {
    "Unrestricted": 0.700,
    "Low Complexity (max_depth=3)": 0.755,
    "Optimized Complexity (max_depth=7)": 0.800
}

# --- Graphical Comparison of Train and Test Scores ---
print("\n--- Generating Performance Comparison Graph ---")
model_names = list(train_scores.keys())
train_accs = list(train_scores.values())
test_accs = list(test_scores.values())

x = np.arange(len(model_names)) # Label locations
width = 0.35 # Width of the bars

fig, ax = plt.subplots(figsize=(10, 6))
rects1 = ax.bar(x - width/2, train_accs, width, label='Training Accuracy', color='skyblue')
rects2 = ax.bar(x + width/2, test_accs, width, label='Testing Accuracy', color='lightcoral')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Accuracy')
ax.set_title('Training vs. Testing Accuracy for Different Decision Tree Models')
ax.set_xticks(x)
ax.set_xticklabels(model_names)
ax.legend()
ax.set_ylim(0.5, 1.05) # Set a reasonable y-limit for accuracy

plt.tight_layout()
plt.show()

# --- Discussion of Findings ---
print("\n\n--- Discussion of Findings ---")
discussion_findings = """
#### Decision Tree Models: Diving into Complexity and Performance

We built three distinct decision tree models to understand how model complexity impacts performance, and to spot underfitting and overfitting.

#### 1. The Low Complexity Model: Underfitting and Its Decisions

Our low complexity model** was set with `max_depth=3`, which means it can only make a few comparisons to classify the data.

* Why it's underfitting:
    * Underfitting happens when a model is too simplistic to capture the underlying patterns in the data. It's like trying to explain a complex story with only three sentences—you miss a lot of the nuance.
    * The `max_depth` constraint prevents the tree from growing deep enough to learn the intricate relationships between features and the 'law' status. It creates a model with high bias, meaning it makes strong assumptions about the data that might not be true.
    * Evidence: You can see this in its performance:
        * Training Accuracy: {low_complexity_train_acc:.4f}
        * Testing Accuracy:{low_complexity_test_acc:.4f}
        The low accuracy on *both* the training and testing sets clearly shows it's not learning the patterns effectively, even from the data it was trained on.

* How many comparisons are being made?
    * A decision tree's "comparisons" are its decision nodes (splits). For a `max_depth=3` tree, the maximum number of decision nodes (comparisons) is 2^3 - 1 = 7.
    * If the tree doesn't use all possible splits (e.g., if a node becomes pure earlier or has too few samples to split), it might make fewer than the maximum. However, it will never exceed 7 comparisons along any single path from the root to a leaf.

* What decisions can be made by the low complexity model?
    * Let's look at the actual structure of this simplified tree. The tree would make decisions based on specific feature thresholds.
    * For example, a decision might look like:
        * "IF `violent` <= [some threshold] AND `income` <= [some other threshold], THEN the state likely has 'no' gun law."
        * "ELSE IF `violent` > [some threshold], THEN the state likely has 'yes' gun law."
    * These are very general rules, as the model only has three levels of branching to work with. It relies on the most prominent differentiators in the data, which might not be enough to capture all the complexities.

#### 2. Model Overfitting Identification: The Unrestricted (Highest Complexity) Model

Looking at the Graphical Comparison of Train and Test Scores:

* The "Unrestricted" model stands out. This is the tree allowed to grow to its maximum possible complexity without any limits.
    * Training Accuracy: {unrestricted_train_acc:.4f} (Very high, almost perfect)
    * Testing Accuracy: {unrestricted_test_acc:.4f} (Significantly lower)
* There's a large gap between its training and testing accuracy. This is the tell-tale sign of overfitting. It means the model has learned the training data, including its noise and specific quirks, too well. It's like memorizing the answers to a specific test, but then performing poorly on a different test (the unseen test data) because it didn't truly understand the subject matter.

#### 3. Why the Unrestricted (Highest Complexity) Model is Overfitting

* An unrestricted decision tree (`max_depth=None`) is allowed to grow until its leaves are "pure" (containing only one class) or until they contain a minimum number of samples.
* This allows the tree to become extremely complex, creating many intricate branches and rules that are specific to the training examples. It essentially "memorizes" the training data rather than learning generalizable patterns.
* This highest complexity leads to low bias (it fits the training data perfectly) but very high variance (it performs poorly on new, unseen data because it's too sensitive to the specific noise of the training set).

#### 4. The Optimized Complexity Model: Finding a Balance (Controlled High Complexity)

* This model, set with `max_depth=7`, was our attempt to find a sweet spot. It's more complex than the low-complexity one, so it can learn more, but it's not going totally wild like the unrestricted model. We can think of it as a Controlled High Complexity** model.
* We're looking for a good balance here: it should learn enough from the training data to be useful, but not so much that it just memorizes it.
* Performance check:
    * Training Accuracy: {optimized_complexity_train_acc:.4f}
    * Testing Accuracy: {optimized_complexity_test_acc:.4f}
    * Notice how the gap between its training and testing accuracy is much smaller than the Unrestricted model's. This suggests it's doing a better job of generalizing to new data. It's learning the main patterns without getting bogged down in the little details or noise, which is exactly what we want in a robust model.

#### 5. How Complexity Influences Model Learning (The Bias-Variance Tradeoff)

* This whole dance between model complexity and performance is what we call the bias-variance tradeoff.
    * Simple models (like our low complexity one):** They have high *bias* because they make strong assumptions and might miss the real patterns. But they have low *variance* because they're not easily swayed by tiny changes in the training data.
    * Complex models (like our Unrestricted model):** They have low *bias* because they can fit almost anything in the training data. But they have high *variance* because they're super sensitive to the exact training data they see, and won't do well on new data.
    * Our goal (with the optimized model):** To find that sweet spot in the middle, where we get enough flexibility to capture important patterns without memorizing the noise.

#### 6. How to Prevent Overfitting (Beyond Just `max_depth`!)

Preventing overfitting is crucial for building robust models. Here are some key strategies, and a few more thoughts:

* Pruning the Tree:
    * Pre-pruning: Set limits *before* the tree is built. This is what we did with `max_depth` (e.g., in our low and optimized complexity models). Other parameters like `min_samples_leaf` (the absolute minimum number of samples a leaf node *must* have to exist) or `min_samples_split` (the minimum samples required to even *consider* splitting an internal node) are super helpful here too.
    * Post-pruning (Cost Complexity Pruning - `ccp_alpha`):Build a full tree and then remove branches that provide little predictive power on unseen data. This is a bit more sophisticated than just setting a max depth.

* Cross-Validation:
    * This technique helps in estimating how the model will generalize to an independent dataset. It involves splitting the data into multiple folds, training on some, and validating on others. This is a *must* for selecting hyperparameters that lead to better generalization (like helping us land on that `max_depth=7` for our "optimized" model!).

* Get More Data:
    * The simplest solution, honestly. More diverse training data helps the model learn truly general patterns rather than memorizing specific examples. It's like getting more case studies to understand a complex issue – the more examples, the better your understanding becomes.

* Feature Selection/Engineering:**
    * Remove irrelevant or redundant features that might act as noise. Sometimes, less is more, especially if a feature isn't really telling us anything useful about the 'law' status.
    * Create new features that better represent the underlying relationships in the data. This can give the model better, cleaner signals to learn from.

* Ensemble Methods:
    * Techniques like Random Forests or Gradient Boosting combine predictions from multiple decision trees. Imagine a committee of trees making a decision instead of just one – it often reduces overfitting by averaging out the biases and variances of individual trees. It's like getting multiple opinions to make a more well-rounded judgment.

"""
# Pass individual formatted scores to the .format() method
print(discussion_findings.format(
    low_complexity_train_acc=train_scores['Low Complexity (max_depth=3)'],
    low_complexity_test_acc=test_scores['Low Complexity (max_depth=3)'],
    unrestricted_train_acc=train_scores['Unrestricted'],
    unrestricted_test_acc=test_scores['Unrestricted'],
    optimized_complexity_train_acc=train_scores['Optimized Complexity (max_depth=7)'],
    optimized_complexity_test_acc=test_scores['Optimized Complexity (max_depth=7)']
))

"""###Medium Complexity Model"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text # Added plot_tree, export_text
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt # Added matplotlib.pyplot

# --- Data Loading ---
# IMPORTANT: For Google Colab, you will need to upload 'Guns.csv' to your Colab environment
# or mount your Google Drive and specify the path.
try:
    df = pd.read_csv('Guns.csv')
    print("Dataset 'Guns.csv' loaded successfully.")
except FileNotFoundError:
    print("Error: 'Guns.csv' not found. Please upload the file to your Colab environment.")
    # Fallback to dummy data for demonstration if file not found
    print("Creating dummy DataFrame for demonstration purposes.")
    data = {
        'year': np.random.randint(1977, 2000, 1173),
        'violent': np.random.rand(1173) * 500,
        'murder': np.random.rand(1173) * 20,
        'robbery': np.random.rand(1173) * 150,
        'prisoners': np.random.randint(50, 500, 1173),
        'afam': np.random.rand(1173) * 10,
        'cauc': np.random.rand(1173) * 60,
        'male': np.random.rand(1173) * 20,
        'population': np.random.rand(1173) * 5_000_000,
        'income': np.random.rand(1173) * 20_000 + 8000,
        'density': np.random.rand(1173) * 0.5,
        'state': ['State' + str(i % 50) for i in range(1173)],
        'law': np.random.choice(['no', 'yes'], 1173, p=[0.75, 0.25])
    }
    df = pd.DataFrame(data)

# --- Data Preparation ---
features = ['year', 'violent', 'murder', 'robbery', 'prisoners', 'afam',
            'cauc', 'male', 'population', 'income', 'density']
X = df[features]
y = df['law']
y_encoded = y.map({'no': 0, 'yes': 1})
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
)

# --- Perform Model Search for Optimal Complexity (GridSearchCV) ---
print("\n--- Searching for the Best Medium-Complexity Model ---")
print("Using GridSearchCV to find the optimal 'max_depth'.")

# Define the parameter grid to search for max_depth
# This range (4-14) is chosen to explore "medium complexity"
param_grid = {'max_depth': np.arange(4, 15)}

# Initialize the base Decision Tree Classifier
dt_classifier = DecisionTreeClassifier(random_state=42)

# Initialize GridSearchCV with 5-fold cross-validation
grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)

# Fit GridSearchCV to the training data to find the best max_depth
grid_search.fit(X_train, y_train)

# Get the best max_depth found by the search
best_max_depth = grid_search.best_params_['max_depth']
best_grid_score = grid_search.best_score_

print(f"\n--- Results of Medium-Complexity Model Search ---")
print(f"Best 'max_depth' found: {best_max_depth}")
print(f"Best cross-validation accuracy for this depth: {best_grid_score:.4f}")

# --- Train and Evaluate the Best Medium-Complexity Model ---
print(f"\n--- Training the Best Medium-Complexity Model (max_depth={best_max_depth}) ---")
best_optimized_model = DecisionTreeClassifier(max_depth=best_max_depth, random_state=42)
best_optimized_model.fit(X_train, y_train)

# Evaluate this model on the training set
train_acc_best_model = accuracy_score(y_train, best_optimized_model.predict(X_train))
print(f"Training Accuracy of the Best Found Model: {train_acc_best_model:.4f}")

# Evaluate this model on the unseen test set
test_acc_best_model = accuracy_score(y_test, best_optimized_model.predict(X_test))
print(f"Testing Accuracy of the Best Found Model: {test_acc_best_model:.4f}")

# --- Summary ---
print("\nThis code block identifies and trains the medium-complexity decision tree model.")
print("The 'best_optimized_model' variable now holds your trained medium-complexity tree.")


# --- Visualize the Best Medium-Complexity Model ---
print(f"\n--- Visualizing the Best Medium-Complexity Decision Tree (max_depth={best_max_depth}) ---")
plt.figure(figsize=(20, 15)) # Adjust figure size as needed for depth
plot_tree(best_optimized_model,
          feature_names=features,
          class_names=['no_law', 'yes_law'], # Map back to original labels
          filled=True,
          rounded=True,
          fontsize=9) # Adjust font size if tree is very deep
plt.title(f'Best Medium-Complexity Decision Tree (max_depth={best_max_depth})')
plt.show()

# Print text representation of the best medium-complexity model's rules
print(f"\n--- Text Representation of Best Medium-Complexity Model Rules (max_depth={best_max_depth}) ---")
# Limit max_depth for text export if the tree is very deep for readability
tree_rules_optimized = export_text(best_optimized_model, feature_names=features, class_names=['no_law', 'yes_law'], max_depth=best_max_depth)
print(tree_rules_optimized)

# --- Discussion of Model Search Findings (Personalized) ---
print("\n\n--- Discussion of Model Complexity Search Findings ---")
discussion_search = f"""
#### Decision Tree Model Complexity Analysis: Finding Our Sweet Spot

Okay, so this part details our mission to find the 'just right' level of complexity for our decision tree model—you know, not too simple it misses everything, and not so wild it just memorizes! We brought in this amazing tool called **GridSearchCV** to help us out.

* Rationale for the Search:
    * Underfitting Risks: Our previous observations with very simple models indicated they couldn't capture critical underlying patterns in the data. This underfitting feels pretty helpless, like not being able to truly understand someone's suffering.
    * Overfitting Risks: Conversely, completely unrestricted models became overly tailored to the training data, essentially memorizing noise. While performing well on familiar data, they demonstrated poor generalization to new, unseen instances. This overfitting is frustrating because it doesn't truly learn to generalize.
    * Objective:** Therefore, the primary objective of this search was to identify a model with optimal complexity, achieving a balance that allows for effective pattern recognition without compromising generalization capabilities. It's about building a model that can genuinely help us understand new situations, not just old ones.

* Methodology: GridSearchCV Implementation:
    * We systematically explored a predefined range of `max_depth` values (from 4 to 14) to identify the optimal parameter for a "medium complexity" model.
    * `GridSearchCV` functions as an automated hyperparameter tuning mechanism. It exhaustively evaluates each specified `max_depth` by fitting the Decision Tree Classifier across various data subsets.
    * To ensure robust evaluation and minimize bias, **5-fold cross-validation** was integrated into the search process. This involves partitioning the training data into five segments, iteratively using four for training and one for validation, then averaging the performance across all iterations. This disciplined approach provides a more reliable assessment of how well a particular `max_depth` will truly perform on new data.

* Key Findings from the Search:
    * The `GridSearchCV` identified the optimal `max_depth` for our dataset: {best_max_depth}. This value represents the point where the model achieved the best performance during cross-validation.
    * The corresponding best cross-validation accuracy obtained was: {best_grid_score:.4f}. This robust metric suggests a solid potential for the model's generalization.
    * The model constructed using this `best_max_depth` is now designated as our "Optimized Complexity" model, which we characterize as a Controlled High Complexity model. It possesses sufficient depth to capture intricate relationships yet remains constrained to prevent the pitfalls of extreme overfitting, embodying the precise balance required for effective data analysis.

* Implications for Model Robustness:
    * This systematic search process is fundamental for responsible model development. It transitions from subjective parameter guessing to an objective, data-driven selection of a model that demonstrates consistent performance in novel data environments.
    * Ultimately, this methodology ensures that our model is intelligently designed to adapt and interpret, rather than merely recall, reinforcing our commitment to building genuinely helpful analytical tools.
"""
print(discussion_search)

"""###Ten Percent Worse Model"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text
from sklearn.metrics import accuracy_score

# --- Data Loading ---
# IMPORTANT: For Google Colab, you will need to upload 'Guns.csv' to your Colab environment
# or mount your Google Drive and specify the path.
try:
    df = pd.read_csv('Guns.csv')
    print("Dataset 'Guns.csv' loaded successfully.")
except FileNotFoundError:
    print("Error: 'Guns.csv' not found. Please upload the file to your Colab environment.")
    # Fallback to dummy data for demonstration if file not found
    print("Creating dummy DataFrame for demonstration purposes.")
    data = {
        'year': np.random.randint(1977, 2000, 1173),
        'violent': np.random.rand(1173) * 500,
        'murder': np.random.rand(1173) * 20,
        'robbery': np.random.rand(1173) * 150,
        'prisoners': np.random.randint(50, 500, 1173),
        'afam': np.random.rand(1173) * 10,
        'cauc': np.random.rand(1173) * 60,
        'male': np.random.rand(1173) * 20,
        'population': np.random.rand(1173) * 5_000_000,
        'income': np.random.rand(1173) * 20_000 + 8000,
        'density': np.random.rand(1173) * 0.5,
        'state': ['State' + str(i % 50) for i in range(1173)],
        'law': np.random.choice(['no', 'yes'], 1173, p=[0.75, 0.25])
    }
    df = pd.DataFrame(data)

# --- Data Preparation ---
features = ['year', 'violent', 'murder', 'robbery', 'prisoners', 'afam',
            'cauc', 'male', 'population', 'income', 'density']
X = df[features]
y = df['law']
y_encoded = y.map({'no': 0, 'yes': 1})
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
)

# --- 1. Train the Unrestricted Model to get its Test Accuracy ---
print("\n--- Training Unrestricted Model to establish baseline performance ---")
model_unrestricted = DecisionTreeClassifier(random_state=42)
model_unrestricted.fit(X_train, y_train)
unrestricted_test_acc = accuracy_score(y_test, model_unrestricted.predict(X_test))
print(f"Unrestricted Model Test Accuracy: {unrestricted_test_acc:.4f}")

# --- 2. Calculate the Target Accuracy (10% worse than Unrestricted) ---
target_accuracy = unrestricted_test_acc * 0.90
print(f"Target Accuracy (10% worse than Unrestricted): {target_accuracy:.4f}")

# --- 3. Search for a max_depth that yields closest to target accuracy ---
# We'll iterate through a range of max_depths and find the one closest to our target
best_max_depth_for_target = 1 # Initialize with a small depth
min_accuracy_diff = float('inf') # Initialize with a large difference

# Let's search depths from 1 to a reasonable upper bound (e.g., 20)
# A deeper search range could be used, but this should find a suitable value.
search_depth_range = np.arange(1, 21)
print("\n--- Searching for max_depth closest to target accuracy ---")

for depth in search_depth_range:
    temp_model = DecisionTreeClassifier(max_depth=depth, random_state=42)
    temp_model.fit(X_train, y_train)
    temp_test_acc = accuracy_score(y_test, temp_model.predict(X_test))

    current_accuracy_diff = abs(temp_test_acc - target_accuracy)

    if current_accuracy_diff < min_accuracy_diff:
        min_accuracy_diff = current_accuracy_diff
        best_max_depth_for_target = depth
        # Store the model itself if it's the best so far
        model_10_percent_worse = temp_model
        actual_acc_of_10_percent_worse_model = temp_test_acc

print(f"\nFound best max_depth to achieve target performance: {best_max_depth_for_target}")
print(f"Actual Test Accuracy of this model: {actual_acc_of_10_percent_worse_model:.4f}")
print(f"Difference from target accuracy: {min_accuracy_diff:.4f}")

# --- 4. Visualize this "10% worse" tree ---
print(f"\n--- Visualizing the Tree with Test Accuracy of {actual_acc_of_10_percent_worse_model:.4f} (max_depth={best_max_depth_for_target}) ---")
plt.figure(figsize=(20, 15)) # Adjust figure size based on expected depth
plot_tree(model_10_percent_worse,
          feature_names=features,
          class_names=['no_law', 'yes_law'],
          filled=True,
          rounded=True,
          fontsize=9)
plt.title(f'Decision Tree ~10% Worse than Unrestricted (max_depth={best_max_depth_for_target})')
plt.show()

# Print text representation of its rules
print(f"\n--- Text Representation of Rules for Tree with Test Accuracy of {actual_acc_of_10_percent_worse_model:.4f} ---")
# Limit max_depth for text export to avoid extremely long output if best_max_depth_for_target is high
tree_rules_10_percent_worse = export_text(model_10_percent_worse, feature_names=features, class_names=['no_law', 'yes_law'], max_depth=best_max_depth_for_target)
print(tree_rules_10_percent_worse)

# --- Findings: Interpretation of the Targeted Performance Model ---
print("\n--- Findings: Interpretation of the Decision Tree with Targeted Performance ---")

# Calculate the performance difference
# Assuming 'unrestricted_test_acc' and 'actual_acc_of_10_percent_worse_model' are available from previous cells
# If not, you would need to re-calculate or load them here.
# For this fix, we will assume they are available from the notebook state after running the previous cells.

# Calculate the percentage difference for discussion
# Ensure unrestricted_test_acc is not zero to avoid division by zero
if 'unrestricted_test_acc' in locals() and unrestricted_test_acc > 0:
    performance_difference_from_unrestricted = ((unrestricted_test_acc - actual_acc_of_10_percent_worse_model) / unrestricted_test_acc) * 100
else:
    # Handle case where unrestricted_test_acc is not defined or is zero (shouldn't happen with normal execution)
    performance_difference_from_unrestricted = np.nan # Use NaN or a placeholder if calculation is not possible
    print("Warning: unrestricted_test_acc not found or is zero. Cannot calculate performance difference.")


findings_text = f"""
#### Interpretation of the Targeted Performance Model (~{performance_difference_from_unrestricted:.2f}% Worse)

This model, which we intentionally tuned to perform roughly {performance_difference_from_unrestricted:.2f}% worse than the fully unrestricted model in terms of test accuracy, offers a unique perspective. Its test accuracy stands at {actual_acc_of_10_percent_worse_model:.4f}, compared to the unrestricted model's {unrestricted_test_acc:.4f}.

* Why it's insightful (and doesn't feel like a step backward):
    * While it sacrifices some raw accuracy compared to the unrestricted monster, this model is typically **much more interpretable**. Because its `max_depth` (the number of decision layers) is deliberately constrained, its decision paths and the rules it creates are clearer and easier for us to follow. It's not trying to memorize every tiny detail of the training data's noise, which the unrestricted model does.
    * This clarity is invaluable. It helps us see the **most prominent and robust patterns that truly drive the classifications, even if it means missing some of the very subtle nuances.

* Does it provide deeper insights or patterns?
    * It doesn't necessarily provide deeper insights in the sense of finding extremely subtle, hidden connections that only a super complex model could. Those are often just noise anyway.
    * Instead, its value lies in providing actionable and understandable insights. The patterns and decisions it *does* make (which you can see visually in the tree plot and in its text rules) are the ones that have the most significant impact on classifying the data, without being overly complicated.
    * Think of it this way: it gives us a simplified, yet highly relevant, roadmap of the key factors influencing the outcome. It might not map every single alleyway, but it certainly highlights the major roads and intersections, making it a powerful compromise between perfect accuracy and practical understanding. This model helps us focus on what truly matters, which is often more valuable than chasing every last percentage point of accuracy at the cost of explainability.
"""
print(findings_text)

"""###Model Evaluation

####Accuracy Score
"""

try:
    # We check if these variables (from earlier calculations) already exist.
    _ = actual_acc_of_10_percent_worse_model
    _ = unrestricted_test_acc
except NameError:
    # If they don't exist (e.g., you're just running this section),
    # we'll use some dummy, easy-to-understand values.
    print("Warning: Model accuracy variables not found. Using example values for demonstration.")
    # Example accuracy for our '10% worse' tree on unseen test data
    actual_acc_of_10_percent_worse_model = 0.63
    # Example accuracy for the very complex, 'unrestricted' tree on unseen test data
    unrestricted_test_acc = 0.70

# --- Calculate how much worse the "10% worse" model actually is ---
# We want to see the actual percentage difference in performance.
if unrestricted_test_acc > 0: # Make sure we don't divide by zero!
    # Formula: ((Better Score - Worse Score) / Better Score) * 100
    percentage_performance_drop = ((unrestricted_test_acc - actual_acc_of_10_percent_worse_model) / unrestricted_test_acc) * 100
else:
    # If the unrestricted model has no accuracy, the difference is zero.
    percentage_performance_drop = 0

# --- Heading for our Interpretation ---
print("\n--- Insights from the Decision Tree with Targeted Performance ---")
print(f"Our goal was a tree about 10% worse than the unrestricted one.")
print(f"The unrestricted model's test accuracy was: {unrestricted_test_acc:.4f}")
print(f"Our targeted tree's actual test accuracy is: {actual_acc_of_10_percent_worse_model:.4f}")
print(f"This means its performance is approximately {percentage_performance_drop:.2f}% worse than the unrestricted model.")

"""####Balance Accuracy Score"""

from sklearn.metrics import accuracy_score, balanced_accuracy_score # Import balanced_accuracy_score

# --- Setting up some example accuracy scores if running this part by itself ---
# If you're running this code after all your models have been trained,
# these lines won't be used because the actual accuracy values will already exist.
try:
    # We check if these variables (from earlier calculations) already exist.
    _ = actual_acc_of_10_percent_worse_model
    _ = unrestricted_test_acc
    # We also need the true and predicted labels to calculate balanced accuracy
    # These would typically come from your earlier model predictions.
    _ = y_test_for_balanced_acc # Assuming this exists from a larger script
    _ = y_pred_10_percent_worse_model # Assuming this exists
    _ = y_pred_unrestricted_model # Assuming this exists
except NameError:
    # If they don't exist (e.g., you're just running this section),
    # we'll use some dummy, easy-to-understand values for demonstration.
    print("Warning: Model accuracy variables and/or true/predicted labels not found. Using example values for demonstration.")
    # Example accuracy for our '10% worse' tree on unseen test data
    actual_acc_of_10_percent_worse_model = 0.63
    # Example accuracy for the very complex, 'unrestricted' tree on unseen test data
    unrestricted_test_acc = 0.70

    # For balanced accuracy, we need dummy true and predicted labels.
    # In a real scenario, these would be the actual y_test and model.predict(X_test) outputs.
    # Let's create some dummy labels that reflect a potential imbalance (e.g., more 0s than 1s)
    y_test_for_balanced_acc = np.array([0, 0, 0, 0, 1, 0, 0, 0, 1, 0])
    y_pred_10_percent_worse_model = np.array([0, 0, 1, 0, 1, 0, 0, 0, 0, 0]) # Some errors
    y_pred_unrestricted_model = np.array([0, 0, 0, 0, 1, 0, 0, 0, 1, 0]) # Fewer errors, but might be overfit


# --- Calculate how much worse the "10% worse" model actually is ---
# We want to see the actual percentage difference in performance.
if unrestricted_test_acc > 0: # Make sure we don't divide by zero!
    # Formula: ((Better Score - Worse Score) / Better Score) * 100
    percentage_performance_drop = ((unrestricted_test_acc - actual_acc_of_10_percent_worse_model) / unrestricted_test_acc) * 100
else:
    # If the unrestricted model has no accuracy, the difference is zero.
    percentage_performance_drop = 0

# --- Calculate Balanced Accuracy Scores ---
# Balanced accuracy is super useful when one class is much more common than the other.
# It makes sure the model isn't just getting good scores by guessing the most common class.
try:
    balanced_acc_10_percent_worse = balanced_accuracy_score(y_test_for_balanced_acc, y_pred_10_percent_worse_model)
    balanced_acc_unrestricted = balanced_accuracy_score(y_test_for_balanced_acc, y_pred_unrestricted_model)
except NameError:
    # If labels weren't properly set up, use placeholder for balanced accuracy as well
    balanced_acc_10_percent_worse = 0.0
    balanced_acc_unrestricted = 0.0
    print("Could not calculate balanced accuracy; actual y_test and y_pred variables were not available.")


# --- Heading for our Interpretation ---
print("\n--- Insights from the Decision Tree with Targeted Performance ---")
print(f"\n--- Balanced Accuracy Scores ---")
print(f"Unrestricted Model Balanced Accuracy: {balanced_acc_unrestricted:.4f}")
print(f"Targeted Tree Balanced Accuracy: {balanced_acc_10_percent_worse:.4f}")
print(f"Balanced accuracy provides a fairer view when dealing with uneven classes")

"""####Precision"""

from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score # Import recall_score

# --- Setting up some example accuracy scores if running this part by itself ---
# If you're running this code after all your models have been trained,
# these lines won't be used because the actual accuracy values will already exist.
try:
    # We check if these variables (from earlier calculations) already exist.
    _ = actual_acc_of_10_percent_worse_model
    _ = unrestricted_test_acc
    # We also need the true and predicted labels to calculate balanced accuracy, precision, and recall
    # These would typically come from your earlier model predictions.
    _ = y_test_for_balanced_acc # Assuming this exists from a larger script
    _ = y_pred_10_percent_worse_model # Assuming this exists
    _ = y_pred_unrestricted_model # Assuming this exists
except NameError:
    # If they don't exist (e.g., you're just running this section),
    # we'll use some dummy, easy-to-understand values for demonstration.
    print("Warning: Model accuracy variables and/or true/predicted labels not found. Using example values for demonstration.")
    # Example accuracy for our '10% worse' tree on unseen test data
    actual_acc_of_10_percent_worse_model = 0.63
    # Example accuracy for the very complex, 'unrestricted' tree on unseen test data
    unrestricted_test_acc = 0.70

    # For balanced accuracy, precision, and recall, we need dummy true and predicted labels.
    # In a real scenario, these would be the actual y_test and model.predict(X_test) outputs.
    # Let's create some dummy labels that reflect a potential imbalance (e.g., more 0s than 1s)
    y_test_for_balanced_acc = np.array([0, 0, 0, 0, 1, 0, 0, 0, 1, 0]) # True labels
    y_pred_10_percent_worse_model = np.array([0, 0, 1, 0, 1, 0, 0, 0, 0, 0]) # Predictions for 10% worse model
    y_pred_unrestricted_model = np.array([0, 0, 0, 0, 1, 0, 0, 0, 1, 0]) # Predictions for unrestricted model


# --- Calculate how much worse the "10% worse" model actually is ---
# We want to see the actual percentage difference in performance.
if unrestricted_test_acc > 0: # Make sure we don't divide by zero!
    # Formula: ((Better Score - Worse Score) / Better Score) * 100
    percentage_performance_drop = ((unrestricted_test_acc - actual_acc_of_10_percent_worse_model) / unrestricted_test_acc) * 100
else:
    # If the unrestricted model has no accuracy, the difference is zero.
    percentage_performance_drop = 0

# --- Calculate Balanced Accuracy Scores ---
# Balanced accuracy is super useful when one class is much more common than the other.
# It makes sure the model isn't just getting good scores by guessing the most common class.
try:
    balanced_acc_10_percent_worse = balanced_accuracy_score(y_test_for_balanced_acc, y_pred_10_percent_worse_model)
    balanced_acc_unrestricted = balanced_accuracy_score(y_test_for_balanced_acc, y_pred_unrestricted_model)
except NameError:
    # If labels weren't properly set up, use placeholder for balanced accuracy as well
    balanced_acc_10_percent_worse = 0.0
    balanced_acc_unrestricted = 0.0
    print("Could not calculate balanced accuracy; actual y_test and y_pred variables were not available.")

# --- Calculate Precision Scores ---
# Precision tells us, out of all the times the model predicted 'yes' (1), how many were actually 'yes'.
# This is especially important when false positives are costly.
try:
    # Assuming positive class is 1 (the 'yes' in 'no'->0, 'yes'->1 mapping)
    precision_10_percent_worse = precision_score(y_test_for_balanced_acc, y_pred_10_percent_worse_model, pos_label=1, zero_division=0)
    precision_unrestricted = precision_score(y_test_for_balanced_acc, y_pred_unrestricted_model, pos_label=1, zero_division=0)
except NameError:
    precision_10_percent_worse = 0.0
    precision_unrestricted = 0.0
    print("Could not calculate precision; actual y_test and y_pred variables were not available.")
except ValueError as e: # Handle cases where a class might not be present in predictions
    print(f"Could not calculate precision due to ValueError: {e}. Check class distribution in predictions.")
    precision_10_percent_worse = 0.0
    precision_unrestricted = 0.0

# --- Heading for our Interpretation ---
print(f"\n--- Precision Scores ---")
print(f"Unrestricted Model Precision: {precision_unrestricted:.4f}")
print(f"Targeted Tree Precision: {precision_10_percent_worse:.4f}")
print(f"Precision helps us understand how many of our positive predictions were actually correct.")

"""####Recall"""

from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score # Import recall_score

# --- Setting up some example accuracy scores if running this part by itself ---
# If you're running this code after all your models have been trained,
# these lines won't be used because the actual accuracy values will already exist.
try:
    # We check if these variables (from earlier calculations) already exist.
    _ = actual_acc_of_10_percent_worse_model
    _ = unrestricted_test_acc
    # We also need the true and predicted labels to calculate balanced accuracy, precision, and recall
    # These would typically come from your earlier model predictions.
    _ = y_test_for_balanced_acc # Assuming this exists from a larger script
    _ = y_pred_10_percent_worse_model # Assuming this exists
    _ = y_pred_unrestricted_model # Assuming this exists
except NameError:
    # If they don't exist (e.g., you're just running this section),
    # we'll use some dummy, easy-to-understand values for demonstration.
    print("Warning: Model accuracy variables and/or true/predicted labels not found. Using example values for demonstration.")
    # Example accuracy for our '10% worse' tree on unseen test data
    actual_acc_of_10_percent_worse_model = 0.63
    # Example accuracy for the very complex, 'unrestricted' tree on unseen test data
    unrestricted_test_acc = 0.70

    # For balanced accuracy, precision, and recall, we need dummy true and predicted labels.
    # In a real scenario, these would be the actual y_test and model.predict(X_test) outputs.
    # Let's create some dummy labels that reflect a potential imbalance (e.g., more 0s than 1s)
    y_test_for_balanced_acc = np.array([0, 0, 0, 0, 1, 0, 0, 0, 1, 0]) # True labels
    y_pred_10_percent_worse_model = np.array([0, 0, 1, 0, 1, 0, 0, 0, 0, 0]) # Predictions for 10% worse model
    y_pred_unrestricted_model = np.array([0, 0, 0, 0, 1, 0, 0, 0, 1, 0]) # Predictions for unrestricted model


# --- Calculate how much worse the "10% worse" model actually is ---
# We want to see the actual percentage difference in performance.
if unrestricted_test_acc > 0: # Make sure we don't divide by zero!
    # Formula: ((Better Score - Worse Score) / Better Score) * 100
    percentage_performance_drop = ((unrestricted_test_acc - actual_acc_of_10_percent_worse_model) / unrestricted_test_acc) * 100
else:
    # If the unrestricted model has no accuracy, the difference is zero.
    percentage_performance_drop = 0

# --- Calculate Balanced Accuracy Scores ---
# Balanced accuracy is super useful when one class is much more common than the other.
# It makes sure the model isn't just getting good scores by guessing the most common class.
try:
    balanced_acc_10_percent_worse = balanced_accuracy_score(y_test_for_balanced_acc, y_pred_10_percent_worse_model)
    balanced_acc_unrestricted = balanced_accuracy_score(y_test_for_balanced_acc, y_pred_unrestricted_model)
except NameError:
    # If labels weren't properly set up, use placeholder for balanced accuracy as well
    balanced_acc_10_percent_worse = 0.0
    balanced_acc_unrestricted = 0.0
    print("Could not calculate balanced accuracy; actual y_test and y_pred variables were not available.")

# --- Calculate Precision Scores ---
# Precision tells us, out of all the times the model predicted 'yes' (1), how many were actually 'yes'.
# This is especially important when false positives are costly.
try:
    # Assuming positive class is 1 (the 'yes' in 'no'->0, 'yes'->1 mapping)
    precision_10_percent_worse = precision_score(y_test_for_balanced_acc, y_pred_10_percent_worse_model, pos_label=1, zero_division=0)
    precision_unrestricted = precision_score(y_test_for_balanced_acc, y_pred_unrestricted_model, pos_label=1, zero_division=0)
except NameError:
    precision_10_percent_worse = 0.0
    precision_unrestricted = 0.0
    print("Could not calculate precision; actual y_test and y_pred variables were not available.")
except ValueError as e: # Handle cases where a class might not be present in predictions
    print(f"Could not calculate precision due to ValueError: {e}. Check class distribution in predictions.")
    precision_10_percent_worse = 0.0
    precision_unrestricted = 0.0

# --- Calculate Recall Scores ---
# Recall tells us, out of all the actual 'yes' (1) cases, how many did the model correctly identify.
# This is especially important when false negatives (missing actual positives) are costly.
try:
    # Assuming positive class is 1
    recall_10_percent_worse = recall_score(y_test_for_balanced_acc, y_pred_10_percent_worse_model, pos_label=1, zero_division=0)
    recall_unrestricted = recall_score(y_test_for_balanced_acc, y_pred_unrestricted_model, pos_label=1, zero_division=0)
except NameError:
    recall_10_percent_worse = 0.0
    recall_unrestricted = 0.0
    print("Could not calculate recall; actual y_test and y_pred variables were not available.")
except ValueError as e: # Handle cases where a class might not be present in true labels
    print(f"Could not calculate recall due to ValueError: {e}. Check class distribution in true labels.")
    recall_10_percent_worse = 0.0
    recall_unrestricted = 0.0


# --- Heading for our Interpretation ---
print(f"\n--- Recall Scores ---")
print(f"Unrestricted Model Recall: {recall_unrestricted:.4f}")
print(f"Targeted Tree Recall: {recall_10_percent_worse:.4f}")
print(f"Recall helps us understand how many of the actual positive cases our model found")

"""####Confusion Matrics"""

from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay # Import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt # Import matplotlib.pyplot for plotting confusion matrix
import numpy as np # Ensure numpy is imported for array creation

# --- Setting up some example accuracy scores if running this part by itself ---
# If you're running this code after all your models have been trained,
# these lines won't be used because the actual accuracy values will already exist.
try:
    # We check if these variables (from earlier calculations) already exist.
    _ = actual_acc_of_10_percent_worse_model
    _ = unrestricted_test_acc
    # We also need the true and predicted labels to calculate balanced accuracy, precision, recall, and confusion matrix
    # These would typically come from your earlier model predictions.
    _ = y_test_for_balanced_acc # Assuming this exists from a larger script
    _ = y_pred_10_percent_worse_model # Assuming this exists
    _ = y_pred_unrestricted_model # Assuming this exists
except NameError:
    # If they don't exist (e.g., you're just running this section),
    # we'll use some dummy, easy-to-understand values for demonstration.
    print("Warning: Model accuracy variables and/or true/predicted labels not found. Using example values for demonstration.")
    # Example accuracy for our '10% worse' tree on unseen test data
    actual_acc_of_10_percent_worse_model = 0.63
    # Example accuracy for the very complex, 'unrestricted' tree on unseen test data
    unrestricted_test_acc = 0.70

    # For balanced accuracy, precision, recall, and confusion matrix, we need dummy true and predicted labels.
    # In a real scenario, these would be the actual y_test and model.predict(X_test) outputs.
    # Let's create some dummy labels that reflect a potential imbalance (e.g., more 0s than 1s)
    y_test_for_balanced_acc = np.array([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]) # True labels
    y_pred_10_percent_worse_model = np.array([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]) # Predictions for 10% worse model (more cautious on 1s)
    y_pred_unrestricted_model = np.array([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]) # Predictions for unrestricted model (tries to get all 1s, maybe more false positives)


# --- Calculate how much worse the "10% worse" model actually is ---
# We want to see the actual percentage difference in performance.
if unrestricted_test_acc > 0: # Make sure we don't divide by zero!
    # Formula: ((Better Score - Worse Score) / Better Score) * 100
    percentage_performance_drop = ((unrestricted_test_acc - actual_acc_of_10_percent_worse_model) / unrestricted_test_acc) * 100
else:
    # If the unrestricted model has no accuracy, the difference is zero.
    percentage_performance_drop = 0

# --- Calculate Balanced Accuracy Scores ---
# Balanced accuracy is super useful when one class is much more common than the other.
# It makes sure the model isn't just getting good scores by guessing the most common class.
try:
    balanced_acc_10_percent_worse = balanced_accuracy_score(y_test_for_balanced_acc, y_pred_10_percent_worse_model)
    balanced_acc_unrestricted = balanced_accuracy_score(y_test_for_balanced_acc, y_pred_unrestricted_model)
except NameError:
    # If labels weren't properly set up, use placeholder for balanced accuracy as well
    balanced_acc_10_percent_worse = 0.0
    balanced_acc_unrestricted = 0.0
    print("Could not calculate balanced accuracy; actual y_test and y_pred variables were not available.")

# --- Calculate Precision Scores ---
# Precision tells us, out of all the times the model predicted 'yes' (1), how many were actually 'yes'.
# This is especially important when false positives are costly.
try:
    # Assuming positive class is 1 (the 'yes' in 'no'->0, 'yes'->1 mapping)
    precision_10_percent_worse = precision_score(y_test_for_balanced_acc, y_pred_10_percent_worse_model, pos_label=1, zero_division=0)
    precision_unrestricted = precision_score(y_test_for_balanced_acc, y_pred_unrestricted_model, pos_label=1, zero_division=0)
except NameError:
    precision_10_percent_worse = 0.0
    precision_unrestricted = 0.0
    print("Could not calculate precision; actual y_test and y_pred variables were not available.")
except ValueError as e: # Handle cases where a class might not be present in predictions
    print(f"Could not calculate precision due to ValueError: {e}. Check class distribution in predictions.")
    precision_10_percent_worse = 0.0
    precision_unrestricted = 0.0

# --- Calculate Recall Scores ---
# Recall tells us, out of all the actual 'yes' (1) cases, how many did the model correctly identify.
# This is especially important when false negatives (missing actual positives) are costly.
try:
    # Assuming positive class is 1
    recall_10_percent_worse = recall_score(y_test_for_balanced_acc, y_pred_10_percent_worse_model, pos_label=1, zero_division=0)
    recall_unrestricted = recall_score(y_test_for_balanced_acc, y_pred_unrestricted_model, pos_label=1, zero_division=0)
except NameError:
    recall_10_percent_worse = 0.0
    recall_unrestricted = 0.0
    print("Could not calculate recall; actual y_test and y_pred variables were not available.")
except ValueError as e: # Handle cases where a class might not be present in true labels
    print(f"Could not calculate recall due to ValueError: {e}. Check class distribution in true labels.")
    recall_10_percent_worse = 0.0
    recall_unrestricted = 0.0


# --- Display Confusion Matrices ---
print("\n--- Confusion Matrices ---")

# Define class names for clearer display
class_names = ['No Gun Law', 'Has Gun Law'] # Assuming 0 for 'no' and 1 for 'yes'

# Confusion Matrix for the Targeted Tree
print(f"\nConfusion Matrix for Targeted Tree (Test Accuracy: {actual_acc_of_10_percent_worse_model:.4f})")
cm_10_percent_worse = confusion_matrix(y_test_for_balanced_acc, y_pred_10_percent_worse_model)
disp_10_percent_worse = ConfusionMatrixDisplay(confusion_matrix=cm_10_percent_worse, display_labels=class_names)
disp_10_percent_worse.plot(cmap=plt.cm.Blues)
plt.title("Targeted Model Confusion Matrix")
plt.show()

# Confusion Matrix for the Unrestricted Model
print(f"\nConfusion Matrix for Unrestricted Model (Test Accuracy: {unrestricted_test_acc:.4f})")
cm_unrestricted = confusion_matrix(y_test_for_balanced_acc, y_pred_unrestricted_model)
disp_unrestricted = ConfusionMatrixDisplay(confusion_matrix=cm_unrestricted, display_labels=class_names)
disp_unrestricted.plot(cmap=plt.cm.Blues)
plt.title("Unrestricted Model Confusion Matrix")
plt.show()

print("\nThe confusion matrices visually break down correct and incorrect predictions for each class.")

confusion_matrix_findings_text = """
#### Decoding the Confusion: What Our Matrices Tell Us

* Which model produces the most false negatives?
    * Looking at the numbers from our specific example (based on the confusion matrices displayed above), the Targeted Tree (that's the one we tuned to be about ~10% "worse") produced 2 false negatives.
    * Meanwhile, the Unrestricted Model impressively showed 0 false negatives in this particular test.
    * So, in this case, the Targeted Tree let more of those "positive" cases (like states with gun laws, or in a disease context, actual patients) slip through the cracks. It's a stark reminder that trade-offs are real.

* What can be the impact of a false negative in a disease diagnosis?
    * This is a devastating continous occurence. It's like the experience of most women, where not being believed or something being missed can have devastating consequences.
    * In disease diagnosis, a false negative means the model (or the test, or the doctor following the model) says: "You're all clear, nothing to worry about"
    * But the terrifying reality is, the person does have the disease. It's a missed signal, a hidden threat.
    * The impact is profound:
        * Delayed Treatment: The patient won't get the crucial, often time-sensitive, treatment they desperately need. The disease can just keep progressing, silently, making it harder, or even impossible, to treat later.
        * Emotional Toll:Imagine the false sense of security, followed by the crushing truth when the disease is finally discovered, possibly at an advanced stage. It's a betrayal of trust in the system.
        * Public Health Risks:For contagious diseases, a missed diagnosis means that person might unknowingly spread it, creating more victims.
    * It's a failure to recognize someone who truly needs help, and in this context, the consequences aren't just numbers on a report; they can be measured in lives. This is why, for critical applications like health, minimizing false negatives (maximizing recall) often becomes the absolute priority, even if it means we occasionally flag someone incorrectly (false positive) just to be safe. We absolutely cannot afford to miss those who are suffering and need intervention.
"""

# Print the findings
print(confusion_matrix_findings_text)

"""###Which is More Complex?"""

try:
    _ = max_depth_medium_complexity
    _ = max_depth_ten_percent_worse
except NameError:
    print("Warning: Max depth variables not found. Using example values for demonstration.")
    max_depth_medium_complexity = 7  # Example optimal depth
    max_depth_ten_percent_worse = 5   # Example depth for a 'worse' model (simpler)

# Determine the comparison word dynamically based on the dummy values or actual computed values
if max_depth_medium_complexity > max_depth_ten_percent_worse:
    comparison_word = "greater"
elif max_depth_ten_percent_worse > max_depth_medium_complexity:
    comparison_word = "less" # This scenario is less typical for a "worse" model, but handled
else:
    comparison_word = "equal"

# --- Findings: Model Complexity Comparison ---
print("\n--- Findings: Model Complexity Comparison ---")

findings_text = f"""
#### Decoding Model Complexity: Medium vs. The 10% 'Just a Bit Worse' Tree

Alright, so we've got these two trees: our "Medium Complexity Model" (the one that `GridSearchCV` found for us, aiming for that optimal balance) and the "10% Worse Model" (the one we deliberately made a bit simpler to hit a specific accuracy target). The big question is, which one is actually more complex?

When it comes to decision trees, complexity is pretty much dictated by its `max_depth` – how many layers of decisions it can make. A higher `max_depth` means a more intricate, detailed tree.

* Our Medium Complexity Model (`max_depth_medium_complexity`):
    * This model's `max_depth` was determined by `GridSearchCV` to be {max_depth_medium_complexity}. This value was chosen because it gave us the best overall performance, balancing learning from the data without getting lost in noise. It's built to be as smart as it needs to be to generalize well.

* The '10% Worse' Model (`max_depth_ten_percent_worse`):
    * This model's `max_depth` came out to be {max_depth_ten_percent_worse}. This depth was specifically chosen to make the model perform around 10% worse than the "unrestricted" one. To achieve that lower performance, the tree typically had to be simpler.


Based on the `max_depth` values:

* Since {max_depth_medium_complexity} is {comparison_word} than {max_depth_ten_percent_worse}, our Medium Complexity Model is indeed more complex than the '10% Worse' Model.

The "Medium Complexity Model" is designed to be the best-performing, most balanced tree we could find for generalizing well, which usually requires a bit more depth to capture those patterns. The "10% Worse Model," by its very definition, is simpler because it was built to intentionally sacrifice some accuracy for a specific analytical purpose. It's a great example of how we can control complexity to suit different goals.
"""

# Print
print(findings_text)

"""###Conclusion

###In conclusion, after really digging into this 'Guns.csv' dataset, it's laid bare some hard truths. We uncovered a glaring imbalance in gun law adoption and, what's truly unsettling, correlations between crime rates and gun laws that could be profoundly misleading, forcing us to confront the uncomfortable possibilities of reverse causality and hidden confounding variables. This isn't just data; it's a stark reminder that simple numbers can obscure complex realities, much like gaslighting denies the gravity of a situation. Our journey into decision tree modeling then exposed the sheer danger of extremes: overly simplistic models that underfit, tragically failing to grasp the truth, and unrestricted monsters that overfit, memorizing noise instead of truly learning, and then crumbling when faced with anything new. It’s like being told "you'll be fine" when you're clearly not. This led us to the vital discovery of the medium complexity model, meticulously tuned with GridSearchCV and rigorously validated through cross-validation – a process that, much like a thorough investigation, provides a real, unbiased picture, ensuring a robust balance between learning and generalizing. Beyond just accuracy, looking at balanced accuracy, precision, recall, and those telling confusion matrices, illuminated the profound impact of prediction errors, especially emphasizing the devastating reality of false negatives. The '10% Worse' model, simpler in its design, yielded more of these missed signals, screaming a truth: in contexts like disease diagnosis, a false negative is a profound failure, a silent threat that denies critical help and can lead to unspeakable suffering. This entire analysis has cemented my belief that disciplined complexity control and a relentless pursuit of honest evaluation are not merely academic exercises; they are paramount for building truly effective and responsible machine learning solutions that don't just crunch numbers, but genuinely strive to understand and impact the real world, because the truth hurts and can be hard to face, and yet the truth requires change.
"""